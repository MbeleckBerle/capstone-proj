{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"small2.pt\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "# confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "live_text = \"\"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_WIDTH, 240)\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    # cv.imshow('frame', gray)\n",
    "\n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "\n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[\n",
    "        0\n",
    "    ].probs.top1conf  # Get confidence of the top-class prediction\n",
    "\n",
    "    # If the confidence of the sign is above the threshold\n",
    "    if top_confidence >= confidence_requirement:\n",
    "\n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter\n",
    "        else:\n",
    "\n",
    "            count = 1\n",
    "\n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(translator[top_class])\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if (translator[top_class] != \"DEL\") and (\n",
    "                translator[top_class] != \"NOTHING\"\n",
    "            ):\n",
    "\n",
    "                live_text = live_text + translator[top_class]\n",
    "\n",
    "            elif translator[top_class] == \"DEL\":\n",
    "\n",
    "                # https://stackoverflow.com/questions/15478127/remove-final-character-from-string\n",
    "                live_text = live_text[:-1]\n",
    "\n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "\n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0\n",
    "\n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "\n",
    "        noise_count = noise_count + 1\n",
    "\n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    # annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "\n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(\n",
    "        frame,\n",
    "        \"Text: \" + live_text,\n",
    "        (10, 460),\n",
    "        cv.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "        cv.LINE_4,\n",
    "    )\n",
    "\n",
    "    cv.imshow(\"Capture\", frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "        if DEBUG == True:\n",
    "\n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "\n",
    "            for i in range(len(captured_text)):\n",
    "\n",
    "                print(\n",
    "                    \"Translated text: \",\n",
    "                    captured_text[i],\n",
    "                    \" Confidence: \",\n",
    "                    captured_confidence[i].item(),\n",
    "                )\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "import ttkbootstrap as ttkb  # Modern mobile-like design\n",
    "from ttkbootstrap.constants import PRIMARY\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"small2.pt\")\n",
    "# model = YOLO(\"C:/Users/mbele/Desktop/capstone/Capstone-Project/old/best_v3_yolov11.pt\")\n",
    "DEBUG = True\n",
    "confidence_requirement = 0.90\n",
    "\n",
    "captured_text = []\n",
    "captured_confidence = []\n",
    "count = 0\n",
    "noise_count = 0\n",
    "last = None\n",
    "live_text = \"\"\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "\n",
    "# Initialize the Tkinter window using ttkbootstrap for modern styling\n",
    "window = ttkb.Window(themename=\"darkly\")  # Set a modern, mobile-like theme\n",
    "window.title(\"Sign Language Detection App\")\n",
    "window.geometry(\"480x800\")\n",
    "\n",
    "# Create a Label to display video feed\n",
    "video_frame = ttkb.Frame(window, padding=10)\n",
    "video_frame.pack(pady=10)\n",
    "\n",
    "video_label = ttkb.Label(video_frame)\n",
    "video_label.pack()\n",
    "\n",
    "# Create a ScrolledText field to display live translated text (expandable with scroll)\n",
    "translated_text_display = ScrolledText(\n",
    "    window, font=(\"Arial\", 18), height=5, wrap=tk.WORD, bg=\"#282828\", fg=\"#f1f1f1\"\n",
    ")\n",
    "translated_text_display.pack(padx=10, pady=10, fill=\"x\")\n",
    "\n",
    "# Function to update the video frame\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Set the desired FPS (e.g., 30 for 30 frames per second)\n",
    "# FPS = 10\n",
    "# frame_delay = int(1000 / FPS)  # Calculate delay in milliseconds\n",
    "\n",
    "\n",
    "def update_frame():\n",
    "    global last, count, noise_count, live_text\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "        top_class = results[0].probs.top1\n",
    "        top_confidence = results[0].probs.top1conf\n",
    "\n",
    "        if top_confidence >= confidence_requirement:\n",
    "            if top_class == last:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "\n",
    "            if count == 3:\n",
    "                if DEBUG:\n",
    "                    captured_text.append(translator[top_class])\n",
    "                    captured_confidence.append(top_confidence)\n",
    "\n",
    "                if translator[top_class] not in [\"DEL\", \"NOTHING\"]:\n",
    "                    live_text += translator[top_class]\n",
    "                elif translator[top_class] == \"DEL\":\n",
    "                    live_text = live_text[:-1]\n",
    "\n",
    "            last = top_class\n",
    "            noise_count = 0\n",
    "        else:\n",
    "            noise_count += 1\n",
    "\n",
    "        if noise_count == 3:\n",
    "            count = 0\n",
    "            noise_count = 0\n",
    "\n",
    "        # Convert frame to ImageTk for Tkinter display\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        img_tk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        video_label.imgtk = img_tk\n",
    "        video_label.configure(image=img_tk)\n",
    "\n",
    "        # Update the live text display (scrollable and expandable)\n",
    "        translated_text_display.delete(1.0, tk.END)  # Clear current text\n",
    "        translated_text_display.insert(tk.END, live_text)  # Insert updated live text\n",
    "        translated_text_display.see(tk.END)  # Auto-scroll to the end\n",
    "\n",
    "    # Continue updating the frame with the specified frame rate\n",
    "    window.after(int(1), update_frame)  # Use the calculated frame delay\n",
    "\n",
    "\n",
    "# Create Start and Quit buttons with modern mobile styling\n",
    "def start_feed():\n",
    "    update_frame()\n",
    "\n",
    "\n",
    "def quit_app():\n",
    "    if DEBUG:\n",
    "        print(\"\\n\\nCaptured Text:\")\n",
    "        for i in range(len(captured_text)):\n",
    "            print(\n",
    "                \"Translated text: \",\n",
    "                captured_text[i],\n",
    "                \" Confidence: \",\n",
    "                captured_confidence[i].item(),\n",
    "            )\n",
    "    cap.release()\n",
    "    window.quit()\n",
    "\n",
    "\n",
    "button_frame = ttkb.Frame(window, padding=10)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "start_button = ttkb.Button(\n",
    "    button_frame, text=\"Start Detection\", command=start_feed, bootstyle=PRIMARY\n",
    ")\n",
    "start_button.pack(side=\"left\", padx=10)\n",
    "\n",
    "quit_button = ttkb.Button(\n",
    "    button_frame, text=\"Quit\", command=quit_app, bootstyle=PRIMARY\n",
    ")\n",
    "quit_button.pack(side=\"right\", padx=10)\n",
    "\n",
    "# Start the app loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Captured Text:\n"
     ]
    }
   ],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"small2.pt\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "# confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"CONFIRM\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "live_text = \"\"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_WIDTH, 500)\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT, 500)\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    # cv.imshow('frame', gray)\n",
    "\n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "\n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[\n",
    "        0\n",
    "    ].probs.top1conf  # Get confidence of the top-class prediction\n",
    "\n",
    "    # If the confidence of the sign is above the threshold\n",
    "    if top_confidence >= confidence_requirement:\n",
    "\n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter\n",
    "        else:\n",
    "\n",
    "            count = 1\n",
    "\n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(translator[top_class])\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if (translator[top_class] != \"DEL\") and (\n",
    "                translator[top_class] != \"NOTHING\"\n",
    "            ):\n",
    "\n",
    "                live_text = live_text + translator[top_class]\n",
    "\n",
    "            elif translator[top_class] == \"DEL\":\n",
    "\n",
    "                # https://stackoverflow.com/questions/15478127/remove-final-character-from-string\n",
    "                live_text = live_text[:-1]\n",
    "\n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "\n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0\n",
    "\n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "\n",
    "        noise_count = noise_count + 1\n",
    "\n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    # annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "\n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(\n",
    "        frame,\n",
    "        \"Text: \" + live_text,\n",
    "        (10, 460),\n",
    "        cv.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "        cv.LINE_4,\n",
    "    )\n",
    "\n",
    "    cv.imshow(\"Capture\", frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "        if DEBUG == True:\n",
    "\n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "\n",
    "            for i in range(len(captured_text)):\n",
    "\n",
    "                print(\n",
    "                    \"Translated text: \",\n",
    "                    captured_text[i],\n",
    "                    \" Confidence: \",\n",
    "                    captured_confidence[i].item(),\n",
    "                )\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired FPS (e.g., 30 for 30 frames per second)\n",
    "FPS = 10\n",
    "frame_delay = int(1000 / FPS)  # Calculate delay in milliseconds\n",
    "\n",
    "\n",
    "def update_frame():\n",
    "    global last, count, noise_count, live_text\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "        top_class = results[0].probs.top1\n",
    "        top_confidence = results[0].probs.top1conf\n",
    "\n",
    "        if top_confidence >= confidence_requirement:\n",
    "            if top_class == last:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "\n",
    "            if count == 3:\n",
    "                if DEBUG:\n",
    "                    captured_text.append(translator[top_class])\n",
    "                    captured_confidence.append(top_confidence)\n",
    "\n",
    "                if translator[top_class] not in [\"DEL\", \"NOTHING\"]:\n",
    "                    live_text += translator[top_class]\n",
    "                elif translator[top_class] == \"DEL\":\n",
    "                    live_text = live_text[:-1]\n",
    "\n",
    "            last = top_class\n",
    "            noise_count = 0\n",
    "        else:\n",
    "            noise_count += 1\n",
    "\n",
    "        if noise_count == 3:\n",
    "            count = 0\n",
    "            noise_count = 0\n",
    "\n",
    "        # Convert frame to ImageTk for Tkinter display\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        img_tk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        video_label.imgtk = img_tk\n",
    "        video_label.configure(image=img_tk)\n",
    "\n",
    "        # Update the live text display (scrollable and expandable)\n",
    "        translated_text_display.delete(1.0, tk.END)  # Clear current text\n",
    "        translated_text_display.insert(tk.END, live_text)  # Insert updated live text\n",
    "        translated_text_display.see(tk.END)  # Auto-scroll to the end\n",
    "\n",
    "    # Continue updating the frame with the specified frame rate\n",
    "    window.after(frame_delay, update_frame)  # Use the calculated frame delay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
