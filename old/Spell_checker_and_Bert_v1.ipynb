{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j1-c-HYUItB"
   },
   "source": [
    "## Spell Checker with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jeffe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KoVXx33UUItC",
    "outputId": "02300de7-51b9-464b-c1e6-3e2ac0cec7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'WORCD' is not a valid word!\n",
      "Suggested correction: world\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load word list dataset\n",
    "word_list = set(words.words())\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Example captured letters\n",
    "captured_text = ['W', 'O', 'R', 'C', 'D',]  # Simulate a misrecognized gesture\n",
    "\n",
    "captured_word = ''.join(captured_text)  \n",
    "\n",
    "# Check if it's a valid word\n",
    "if captured_word.lower() in word_list:\n",
    "    print(f\"'{captured_word}' is a valid word!\")\n",
    "else:\n",
    "    print(f\"'{captured_word}' is not a valid word!\")\n",
    "\n",
    "    # Suggest correction\n",
    "    corrected_word = spell.correction(captured_word)\n",
    "    print(f\"Suggested correction: {corrected_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRVXZX2uUItC"
   },
   "source": [
    "# Joining Words in a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VC7nVT54UItC",
    "outputId": "5eef280a-5a56-4049-a2f3-13fce25955ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'WORCD' is not a valid word. Corrected to 'world'.\n",
      "'GREBT' is not a valid word. Corrected to 'great'.\n",
      "\n",
      "Final Sentence: world is great\n"
     ]
    }
   ],
   "source": [
    "# Load word list dataset\n",
    "word_list = set(words.words())\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Example captured letters\n",
    "captured_sequences = ['W', 'O', 'R', 'C', 'D', ' ', 'I', 'S', ' ', 'G', 'R', 'E', 'B', 'T']  # Simulate a misrecognized gesture with spaces\n",
    "\n",
    "# Helper function to check if a word is valid\n",
    "def is_valid_word(word):\n",
    "    return word.lower() in word_list\n",
    "\n",
    "# Process the captured sequence\n",
    "sentence = \"\"\n",
    "current_word = \"\"\n",
    "\n",
    "for letter in captured_sequences:\n",
    "    if letter == ' ':\n",
    "        # Check and correct the current word, then reset\n",
    "        if current_word:\n",
    "            if not is_valid_word(current_word):\n",
    "                corrected_word = spell.correction(current_word.lower())\n",
    "                sentence += corrected_word.lower() + \" \"  # Convert to lowercase and add space\n",
    "                print(f\"'{current_word}' is not a valid word. Corrected to '{corrected_word}'.\")\n",
    "            else:\n",
    "                sentence += current_word.lower() + \" \"  # Add space after the valid word\n",
    "            current_word = \"\"\n",
    "    else:\n",
    "        current_word += letter\n",
    "\n",
    "# Check and correct the last word (if any)\n",
    "if current_word:\n",
    "    if not is_valid_word(current_word):\n",
    "        corrected_word = spell.correction(current_word.lower())\n",
    "        sentence += corrected_word.lower()  # Convert to lowercase\n",
    "        print(f\"'{current_word}' is not a valid word. Corrected to '{corrected_word}'.\")\n",
    "    else:\n",
    "        sentence += current_word.lower()  # Convert to lowercase\n",
    "\n",
    "print(\"\\nFinal Sentence:\", sentence.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn19ktlIa3I8"
   },
   "source": [
    "### Correcting with context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeYV8i5Pkm9o",
    "outputId": "7af531f2-346e-4b3f-f93c-f6f80e41560d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jeffe\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\jeffe\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize spell checker and fill-mask pipeline\n",
    "spell = SpellChecker()\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VES89AgmC5P",
    "outputId": "c7cc92ef-247d-405e-fe21-e9dcbdf18a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Many', 'issues', 'come', 'from', 'a', 'poor', 'diet', 'For', 'example', 'sweet', 'can', 'effect', 'your', 'health']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initial spell correction\n",
    "input_sentence = \"Many issues come from a poor diet. For example, sweet can xffect your health.\"\n",
    "words = input_sentence.split()\n",
    "\n",
    "corrected_words = []\n",
    "for word in words:\n",
    "    if word.lower():\n",
    "        # Spell correction step\n",
    "        corrected = spell.correction(word)\n",
    "        corrected_words.append(corrected if corrected else word)  # If no correction, keep original\n",
    "    else:\n",
    "        corrected_words.append(word)\n",
    "\n",
    "print(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The mispealled word \"xffect\" was corrected to \"effect\" - \"effect\" is more common than \"affect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUrghz8NmvBD",
    "outputId": "f788609c-1fd7-4255-bb17-b3b22d3b159d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidates: ['affect']\n",
      "Corrected Sentence: Many issues come from a poor diet For example sweet can affect your health\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Contextual prediction on the sentence\n",
    "context_sentence = \" \".join(corrected_words).replace(\"effect\", \"[MASK]\").replace(\"affect\", \"[MASK]\")\n",
    "predictions = fill_mask(context_sentence)\n",
    "\n",
    "# Filter predictions for \"affect\" or \"effect\"\n",
    "candidates = [pred[\"token_str\"] for pred in predictions if pred[\"token_str\"] in [\"affect\", \"effect\"]]\n",
    "\n",
    "# Final sentence adjustment\n",
    "if candidates:\n",
    "    corrected_sentence = context_sentence.replace(\"[MASK]\", candidates[0])\n",
    "else:\n",
    "    corrected_sentence = \" \".join(corrected_words)  # If no relevant candidates, use spell-check output\n",
    "\n",
    "print(\"Top candidates:\", candidates)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually replace \"effect\" for [MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAYBWSB-sb1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k62hQ0djsb3_"
   },
   "source": [
    "## Using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "AZTDXwYawrEf",
    "outputId": "0762715d-3e11-438d-f492-e72001c1e857"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidates: ['help', 'improve', 'affect', 'hurt', 'harm']\n",
      "Corrected Sentence: Many issues come from a poor diet. For example, sweet can help your health.\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "# Initialize spell checker and fill-mask pipeline\n",
    "spell = SpellChecker()\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Function to get confusing words from Datamuse API\n",
    "#def get_confusing_words(word):\n",
    "  #  url = f\"https://api.datamuse.com/words?rel_hom={word}\"\n",
    "  #  response = requests.get(url)\n",
    "  #  confusing_words = [item['word'] for item in response.json()]\n",
    "  #  return confusing_words if confusing_words else [word]\n",
    "\n",
    "# Step 1: Build Confusing Words Dictionary for Input Sentence\n",
    "#def build_confusing_word_dict(text):\n",
    "  #  words = set(text.split())  # Extract unique words\n",
    "  #  confusing_word_dict = {}\n",
    "\n",
    "  #  for word in words:\n",
    "  #      confusing_word_dict[word] = get_confusing_words(word)\n",
    "\n",
    "  #  return confusing_word_dict\n",
    "\n",
    "# Step 2: Initial spell correction with Confusing Words Dictionary\n",
    "input_sentence = \"Many issues come from a poor diet. For example, sweet can xffect your health.\"\n",
    "confusing_word_dict = build_confusing_word_dict(input_sentence)\n",
    "\n",
    "corrected_words = []\n",
    "for word in input_sentence.split():\n",
    "    if word.lower() in confusing_word_dict:\n",
    "        # Use spell correction and check against confusing words\n",
    "        corrected = spell.correction(word)\n",
    "        # If the corrected word is in confusing word dictionary, add to list\n",
    "        if corrected and corrected in confusing_word_dict[word.lower()]:\n",
    "            corrected_words.append(corrected)\n",
    "        else:\n",
    "            corrected_words.append(word)  # Keep original if no correction\n",
    "    else:\n",
    "        corrected_words.append(word)\n",
    "\n",
    "# Step 3: Contextual prediction on the corrected sentence\n",
    "context_sentence = \" \".join(corrected_words).replace(\"xffect\", \"[MASK]\")  # replace xffect with [MASK]\n",
    "\n",
    "# Check if there's a [MASK] token in context_sentence before proceeding\n",
    "if \"[MASK]\" in context_sentence:\n",
    "    predictions = fill_mask(context_sentence)\n",
    "\n",
    "    # Filter predictions for \"affect\" or \"effect\"\n",
    "    candidates = [pred[\"token_str\"] for pred in predictions if pred[\"token_str\"] ]\n",
    "                  \n",
    "\n",
    "    # Final sentence adjustment\n",
    "    if candidates:\n",
    "        corrected_sentence = context_sentence.replace(\"[MASK]\", candidates[0])\n",
    "    else:\n",
    "        corrected_sentence = \" \".join(corrected_words)  # If no relevant candidates, use spell-check output\n",
    "else:\n",
    "    corrected_sentence = \" \".join(corrected_words)  # If no [MASK] token, skip fill-mask\n",
    "\n",
    "print(\"Top candidates:\", candidates if 'candidates' in locals() else \"No relevant candidates found\")\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT is changing effect for help. It does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidates: ['ruin', 'hurt', 'harm', 'affect', 'damage']\n",
      "Corrected Sentence: Too much sugar may ruin your health.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initial spell correction with Confusing Words Dictionary\n",
    "input_sentence = \"Too much sugar may xffect your health.\"\n",
    "confusing_word_dict = build_confusing_word_dict(input_sentence)\n",
    "\n",
    "corrected_words = []\n",
    "for word in input_sentence.split():\n",
    "    if word.lower() in confusing_word_dict:\n",
    "        # Use spell correction and check against confusing words\n",
    "        corrected = spell.correction(word)\n",
    "        # If the corrected word is in confusing word dictionary, add to list\n",
    "        if corrected and corrected in confusing_word_dict[word.lower()]:\n",
    "            corrected_words.append(corrected)\n",
    "        else:\n",
    "            corrected_words.append(word)  # Keep original if no correction\n",
    "    else:\n",
    "        corrected_words.append(word)\n",
    "\n",
    "# Step 3: Contextual prediction on the corrected sentence\n",
    "context_sentence = \" \".join(corrected_words).replace(\"xffect\", \"[MASK]\")  # replace xffect with [MASK]\n",
    "\n",
    "# Check if there's a [MASK] token in context_sentence before proceeding\n",
    "if \"[MASK]\" in context_sentence:\n",
    "    predictions = fill_mask(context_sentence)\n",
    "\n",
    "    # Filter predictions for \"affect\" or \"effect\"\n",
    "    candidates = [pred[\"token_str\"] for pred in predictions if pred[\"token_str\"] ]\n",
    "                  \n",
    "\n",
    "    # Final sentence adjustment\n",
    "    if candidates:\n",
    "        corrected_sentence = context_sentence.replace(\"[MASK]\", candidates[0])\n",
    "    else:\n",
    "        corrected_sentence = \" \".join(corrected_words)  # If no relevant candidates, use spell-check output\n",
    "else:\n",
    "    corrected_sentence = \" \".join(corrected_words)  # If no [MASK] token, skip fill-mask\n",
    "\n",
    "print(\"Top candidates:\", candidates if 'candidates' in locals() else \"No relevant candidates found\")\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing the text may change completely the top candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next steps: figure out what is happening behind to get these Top Candidates Results. \n",
    "\n",
    "# First example\n",
    "# Top candidates: ['help', 'improve', 'affect', 'hurt', 'harm']\n",
    "\n",
    "# Second example\n",
    "# Top candidates: ['ruin', 'hurt', 'harm', 'affect', 'damage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamuse API to get homophones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commonly confused words for 'effect': ['affect']\n",
      "Commonly confused words for 'desert': ['dessert']\n",
      "Commonly confused words for 'base': ['bass']\n"
     ]
    }
   ],
   "source": [
    "# Fetch homophones from Datamuse API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch homophones from Datamuse API\n",
    "def get_homophones(word):\n",
    "    response = requests.get(f\"https://api.datamuse.com/words?rel_hom={word}\")\n",
    "    homophones = [entry['word'] for entry in response.json()]\n",
    "    return homophones\n",
    "\n",
    "# Example usage\n",
    "word = \"effect\"\n",
    "homophones = get_homophones(word)\n",
    "print(f\"Commonly confused words for '{word}':\", homophones)\n",
    "\n",
    "\n",
    "word2 = \"desert\"\n",
    "homophones = get_homophones(word2)\n",
    "print(f\"Commonly confused words for '{word2}':\", homophones)\n",
    "\n",
    "word3 = \"base\"\n",
    "homophones = get_homophones(word3)\n",
    "print(f\"Commonly confused words for '{word3}':\", homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially confusing words for 'diet': ['die it', 'dye it']\n",
      "Potentially confusing words for 'effect': ['affect']\n",
      "Corrected Sentence: Many issues come from a poor . For example sweet can help your health\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize spell checker and fill-mask pipeline\n",
    "spell = SpellChecker()\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Function to get homophones from Datamuse API\n",
    "def get_homophones(word):\n",
    "    response = requests.get(f\"https://api.datamuse.com/words?rel_hom={word}\")\n",
    "    homophones = [entry['word'] for entry in response.json()]\n",
    "    return homophones\n",
    "\n",
    "# Function to check and correct words\n",
    "def correct_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "\n",
    "    for word in words:\n",
    "        # Check spelling\n",
    "        if word.lower() in word_list:  # Assuming word_list is defined with valid words\n",
    "            corrected_sentence.append(word)\n",
    "        else:\n",
    "            # Suggest correction\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_sentence.append(corrected_word)\n",
    "\n",
    "            # Get homophones\n",
    "            homophones = get_homophones(corrected_word)\n",
    "            if homophones:\n",
    "                print(f\"Potentially confusing words for '{corrected_word}': {homophones}\")\n",
    "\n",
    "                # Replace word with a mask for BERT\n",
    "                masked_sentence = sentence.replace(word, \"[MASK]\")\n",
    "                \n",
    "                # Apply BERT to predict the best word\n",
    "                predictions = fill_mask(masked_sentence)\n",
    "\n",
    "                # Replace the mask with the best prediction\n",
    "                if predictions:\n",
    "                    best_candidate = predictions[0]['token_str']\n",
    "                    corrected_sentence[-1] = best_candidate  # Replace last word with prediction\n",
    "\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"Many issues come from a poor diet. For example, sweet can xffect your health.\"\n",
    "corrected = correct_sentence(input_sentence)\n",
    "print(\"Corrected Sentence:\", corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same results as before without Datamuse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
