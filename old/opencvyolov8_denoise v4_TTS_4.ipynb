{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Captured Text:\n",
      "Translated text:  Y  Confidence:  0.9036934971809387\n",
      "Translated text:  E  Confidence:  0.9673931002616882\n",
      "Translated text:  C  Confidence:  0.9869916439056396\n",
      "Translated text:  DEL  Confidence:  0.9715714454650879\n",
      "Translated text:  DEL  Confidence:  0.9285758137702942\n",
      "Translated text:  DEL  Confidence:  0.9236694574356079\n",
      "Translated text:  NOTHING  Confidence:  0.9993591904640198\n",
      "Translated text:  NOTHING  Confidence:  0.9765427708625793\n",
      "Translated text:  NOTHING  Confidence:  0.9963998794555664\n",
      "Translated text:  NOTHING  Confidence:  0.9248594045639038\n",
      "Translated text:  NOTHING  Confidence:  0.998508632183075\n",
      "Translated text:  NOTHING  Confidence:  0.9994008541107178\n",
      "Translated text:  NOTHING  Confidence:  0.9205377101898193\n",
      "Translated text:  P  Confidence:  0.9478135704994202\n",
      "Translated text:  G  Confidence:  0.9944655299186707\n",
      "Translated text:  T  Confidence:  0.9780483245849609\n",
      "Translated text:  T  Confidence:  0.9223892092704773\n",
      "Translated text:  NOTHING  Confidence:  0.9821373820304871\n",
      "Translated text:  C  Confidence:  0.9782630801200867\n",
      "Speaking:  YECDELDELDELNOTHINGNOTHINGNOTHINGNOTHINGNOTHINGNOTHINGNOTHINGPGTTNOTHINGC\n"
     ]
    }
   ],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "# pyttsx3 courtesy of: https://pypi.org/project/pyttsx3/\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pyttsx3\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Initialize pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# set speech speed rate\n",
    "default_rate = engine.getProperty(\"rate\")\n",
    "engine.setProperty(\"rate\", int(default_rate * 0.75))  # set the speed rate to 75%\n",
    "\n",
    "\n",
    "# select voice gender\n",
    "def select_voice(engine, gender=\"m\"):\n",
    "    voices = engine.getProperty(\"voices\")\n",
    "    if gender == \"f\":\n",
    "        engine.setProperty(\"voice\", voices[1].id)  # 1 for female\n",
    "    else:\n",
    "        engine.setProperty(\"voice\", voices[0].id)  # 0 for male\n",
    "\n",
    "\n",
    "# prompt to select gender\n",
    "gender = input(\"Select voice (m: male / f: female) : \").strip().lower()\n",
    "select_voice(engine, gender)\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"best_v3_yolov11.pt\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "# confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \" \",\n",
    "]\n",
    "live_text = \"Text: \"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_WIDTH,240)\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT,240)\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    # cv.imshow('frame', gray)\n",
    "\n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "\n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[\n",
    "        0\n",
    "    ].probs.top1conf  # Get confidence of the top-class prediction\n",
    "\n",
    "    # If the confidence of the sign is above the threshold\n",
    "    if top_confidence >= confidence_requirement:\n",
    "\n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter\n",
    "        else:\n",
    "\n",
    "            count = 1\n",
    "\n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "            sign = translator[top_class]\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(sign)\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if sign == \"SPACE\":\n",
    "                captured_text.append(\"_\")  # Store \"_\" for space in captured_text\n",
    "                live_text += \"_\"  # Display \"_\" instead of \" \" in live_text\n",
    "\n",
    "            elif sign == \"DEL\":\n",
    "                live_text = live_text[:-1]  # Removes the last character\n",
    "\n",
    "            elif sign != \"NOTHING\":\n",
    "                live_text += sign  # Appends the recognized gesture to live_text\n",
    "\n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "\n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0\n",
    "\n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "\n",
    "        noise_count += 1\n",
    "\n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    # annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "\n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(\n",
    "        frame,\n",
    "        live_text,\n",
    "        (10, 460),\n",
    "        cv.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "        cv.LINE_4,\n",
    "    )\n",
    "\n",
    "    cv.imshow(\"Capture\", frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "        if DEBUG == True:\n",
    "\n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "\n",
    "            for i in range(len(captured_text)):\n",
    "\n",
    "                print(\n",
    "                    \"Translated text: \",\n",
    "                    captured_text[i],\n",
    "                    \" Confidence: \",\n",
    "                    captured_confidence[i].item(),\n",
    "                )\n",
    "\n",
    "        # now I replaced the '_' with ' ' when converted to text\n",
    "        spoken_text = \"\".join(captured_text).replace(\"_\", \" \").strip()\n",
    "        print(\"Speaking: \", spoken_text)\n",
    "        engine.say(spoken_text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# a function to replay the captured text after ending the webcam feed\n",
    "while True:\n",
    "    replay_choice = (\n",
    "        input(\"Press 'R' to replay the sentence or 'E' to exit: \").strip().lower()\n",
    "    )\n",
    "    if replay_choice == \"r\":\n",
    "        print(\"Replaying: \", spoken_text)\n",
    "        engine.say(spoken_text)\n",
    "        engine.runAndWait()\n",
    "    elif replay_choice == \"e\":\n",
    "        print(\"Exiting.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pre-load and resize all images to 500x500\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m letter, path \u001b[38;5;129;01min\u001b[39;00m asl_image_paths\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 39\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241m.\u001b[39mimread(path)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         resized_img \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "# Courtesy of: https://www.geeksforgeeks.org/essential-opencv-functions-to-get-started-into-computer-vision/\n",
    "\n",
    "# Mapping each letter+space to the corresponding ASL image path\n",
    "asl_image_paths = {\n",
    "    \"A\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\A.jpg\",\n",
    "    \"B\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\B.jpg\",\n",
    "    \"C\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\C.jpg\",\n",
    "    \"D\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\D.jpg\",\n",
    "    \"E\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\E.jpg\",\n",
    "    \"F\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\F.jpg\",\n",
    "    \"G\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\G.jpg\",\n",
    "    \"H\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\H.jpg\",\n",
    "    \"I\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\I.jpg\",\n",
    "    \"J\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\J.jpg\",\n",
    "    \"K\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\K.jpg\",\n",
    "    \"L\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\L.jpg\",\n",
    "    \"M\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\M.jpg\",\n",
    "    \"N\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\N.jpg\",\n",
    "    \"O\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\O.jpg\",\n",
    "    \"P\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\P.jpg\",\n",
    "    \"Q\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Q.jpg\",\n",
    "    \"R\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\R.jpg\",\n",
    "    \"S\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\S.jpg\",\n",
    "    \"T\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\T.jpg\",\n",
    "    \"U\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\U.jpg\",\n",
    "    \"V\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\V.jpg\",\n",
    "    \"W\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\W.jpg\",\n",
    "    \"X\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\X.jpg\",\n",
    "    \"Y\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Y.jpg\",\n",
    "    \"Z\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Z.jpg\",\n",
    "    \"_\": r\"C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\SPACE.jpg\",\n",
    "}\n",
    "\n",
    "# a dictionary to store loaded and resized images to avoid loading each time\n",
    "asl_images = {}\n",
    "\n",
    "# pre-load and resize all images to 500x500\n",
    "for letter, path in asl_image_paths.items():\n",
    "    img = cv.imread(path)\n",
    "    if img is not None:\n",
    "        resized_img = cv.resize(img, (500, 500))\n",
    "        asl_images[letter] = resized_img\n",
    "    else:\n",
    "        print(f\"The image for '{letter}' is not found at : {path}\")\n",
    "\n",
    "\n",
    "# a function to display each letter as an ASL gesture\n",
    "def display_asl_gesture(text):\n",
    "    for char in text:\n",
    "        # again, convert the space (' ') to '_' for the dictionary\n",
    "        char = \"_\" if char == \" \" else char.upper()\n",
    "\n",
    "        # get the pre-loaded and resized ASL image\n",
    "        img = asl_images.get(char)\n",
    "        if img is not None:\n",
    "            cv.imshow(f\"ASL Gesture for {char}\", img)\n",
    "            cv.waitKey(1000)  # set the display time for each image to 1 second\n",
    "            cv.destroyWindow(f\"ASL Gesture for {char}\")\n",
    "        else:\n",
    "            print(f\"ASL gesture for '{char}' is not available.\")\n",
    "\n",
    "\n",
    "# input text to be translated to ASL\n",
    "input_text = input(\"Enter text to translate to ASL gestures: \").strip()\n",
    "\n",
    "# call the display ASL function\n",
    "display_asl_gesture(input_text)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation using google translator </br> link:https://deep-translator.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "languages = requests.get(\"https://ws.detectlanguage.com/0.2/languages\")\n",
    "languages.json()  # displays the codes for various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Jim. Tu es superbe aujourd'hui\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "def translate(text, target_language):\n",
    "    translated_text = GoogleTranslator(source=\"auto\", target=target_language).translate(\n",
    "        text\n",
    "    )\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "text_input = input(\"Enter text\")  # Replace with extracted text from the sign language\n",
    "target_lang = input(\"Enter target language\")  # replace with target language\n",
    "\n",
    "translated_text = translate(text_input, target_lang)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"O sol mergulhou abaixo do horizonte, lançando um brilho quente no céu. Conforme as estrelas começaram a brilhar, a brisa suave carregava o perfume das flores desabrochando. À distância, o riso ecoava de uma reunião próxima, onde amigos compartilhavam histórias e criavam memórias. Era uma noite perfeita, cheia de promessas de aventura e da beleza das maravilhas da natureza.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"غابت الشمس خلف الأفق، فألقت ضوءًا دافئًا عبر السماء. ومع بدء النجوم في التألق، حملت النسمة اللطيفة رائحة الزهور المتفتحة. وفي المسافة، تردد صدى الضحك من تجمع قريب، حيث تبادل الأصدقاء القصص وخلقوا الذكريات. لقد كانت أمسية مثالية، مليئة بوعد المغامرة وجمال عجائب الطبيعة.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sun dipped below the horizon, casting a warm glow across the sky. As the stars began to twinkle, the gentle breeze carried the scent of blooming flowers. In the distance, laughter echoed from a nearby gathering, where friends shared stories and created memories. It was a perfect evening, filled with the promise of adventure and the beauty of nature's wonders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Owia no kɔhyɛɛ wim ase, na ɛmaa wim hyerɛn denneennen. Bere a nsoromma fii ase hyerɛn no, mframa a ɛbɔ brɛoo no kuraa nhwiren a ɛrefefɛw hua. Wɔ akyirikyiri no, na serew gyegyeegye fii nhyiam bi a ɛbɛn hɔ mu, faako a nnamfo kaa nsɛm na wɔbɔɔ nkae. Na ɛyɛ anwummere a edi mũ, a na bɔhyɛ a ɛfa akwantu ho ne abɔde mu anwonwade ahorow a ɛyɛ fɛ ahyɛ mu ma.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
