{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "live_text = \"Hello Jim, how are you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextWord(text=\"\"):\n",
    "\n",
    "    test = generator(live_text, max_new_tokens=1, num_return_sequences=3)\n",
    "\n",
    "    # Info about getting last word from string\n",
    "    # https://stackoverflow.com/questions/67615195/python-how-to-get-the-last-word-from-a-sentence-in-python\n",
    "    \n",
    "    return test[0]['generated_text'].split()[-1], test[1]['generated_text'].split()[-1], test[2]['generated_text'].split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3 = nextWord(live_text)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to install the following\n",
    "# pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "0.18.0+cu121\n"
     ]
    }
   ],
   "source": [
    "#pyttsx3 courtesy of: https://pypi.org/project/pyttsx3/\n",
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# set speech speed rate\n",
    "default_rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', int(default_rate * 0.75))  # set the speed rate to 75%\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# changed this to default choose female.\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "# Use UI to change this\n",
    "# select voice gender\n",
    "def select_voice(engine, gender='f'):\n",
    "    voices = engine.getProperty('voices')\n",
    "    if gender == 'f':\n",
    "        engine.setProperty('voice', voices[1].id)  #1 for female\n",
    "    else:\n",
    "        engine.setProperty('voice', voices[0].id)  #0 for male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# prompt to select gender\\ngender = input(\"Select voice (m: male / f: female) : \").strip().lower()\\nselect_voice(engine, gender)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build this into UI\n",
    "\"\"\"\n",
    "# prompt to select gender\n",
    "gender = input(\"Select voice (m: male / f: female) : \").strip().lower()\n",
    "select_voice(engine, gender)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToSpeech(text=\"Hello\"):\n",
    "\n",
    "    # now I replaced the '_' with ' ' when converted to text\n",
    "    spoken_text = ''.join(text).replace(\"_\", \" \").strip()\n",
    "    print(\"Speaking: \", spoken_text)\n",
    "    engine.say(spoken_text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaking:  How are you doing?\n"
     ]
    }
   ],
   "source": [
    "textToSpeech(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd this at the end of yolo code to allow replayability or add to UI\\n# a function to replay the captured text after ending the webcam feed\\nwhile True:\\n    replay_choice = input(\"Press \\'R\\' to replay the sentence or \\'E\\' to exit: \").strip().lower()\\n    if replay_choice == \\'r\\':\\n        print(\"Replaying: \", spoken_text)\\n        engine.say(spoken_text)\\n        engine.runAndWait()\\n    elif replay_choice == \\'e\\':\\n        print(\"Exiting.\")\\n        break\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add this at the end of yolo code to allow replayability or add to UI\n",
    "\n",
    "# a function to replay the captured text after ending the webcam feed\n",
    "while True:\n",
    "    replay_choice = input(\"Press 'R' to replay the sentence or 'E' to exit: \").strip().lower()\n",
    "    if replay_choice == 'r':\n",
    "        print(\"Replaying: \", spoken_text)\n",
    "        engine.say(spoken_text)\n",
    "        engine.runAndWait()\n",
    "    elif replay_choice == 'e':\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to ASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Courtesy of: https://www.geeksforgeeks.org/essential-opencv-functions-to-get-started-into-computer-vision/\n",
    "\n",
    "# Mapping each letter+space to the corresponding ASL image path\n",
    "asl_image_paths = {\n",
    "    'A': r'ASL TTS Dataset\\A.jpg',\n",
    "    'B': r'ASL TTS Dataset\\B.jpg',\n",
    "    'C': r'ASL TTS Dataset\\C.jpg',\n",
    "    'E': r'ASL TTS Dataset\\E.jpg',\n",
    "    'F': r'ASL TTS Dataset\\F.jpg',\n",
    "    'G': r'ASL TTS Dataset\\G.jpg',\n",
    "    'H': r'ASL TTS Dataset\\H.jpg',\n",
    "    'I': r'ASL TTS Dataset\\I.jpg',\n",
    "    'J': r'ASL TTS Dataset\\J.jpg',\n",
    "    'K': r'ASL TTS Dataset\\K.jpg',\n",
    "    'L': r'ASL TTS Dataset\\L.jpg',\n",
    "    'M': r'ASL TTS Dataset\\M.jpg',\n",
    "    'N': r'ASL TTS Dataset\\N.jpg',\n",
    "    'O': r'ASL TTS Dataset\\O.jpg',\n",
    "    'P': r'ASL TTS Dataset\\P.jpg',\n",
    "    'Q': r'ASL TTS Dataset\\Q.jpg',\n",
    "    'R': r'ASL TTS Dataset\\R.jpg',\n",
    "    'S': r'ASL TTS Dataset\\S.jpg',\n",
    "    'T': r'ASL TTS Dataset\\T.jpg',\n",
    "    'U': r'ASL TTS Dataset\\U.jpg',\n",
    "    'V': r'ASL TTS Dataset\\V.jpg',\n",
    "    'W': r'ASL TTS Dataset\\W.jpg',\n",
    "    'X': r'ASL TTS Dataset\\X.jpg',\n",
    "    'Y': r'ASL TTS Dataset\\Y.jpg',\n",
    "    'Z': r'ASL TTS Dataset\\Z.jpg',\n",
    "    '_': r'ASL TTS Dataset\\SPACE.jpg'\n",
    "}\n",
    "\n",
    "# a dictionary to store loaded and resized images to avoid loading each time\n",
    "asl_images = {}\n",
    "\n",
    "# pre-load and resize all images to 500x500\n",
    "for letter, path in asl_image_paths.items():\n",
    "    img = cv.imread(path)\n",
    "    if img is not None:\n",
    "        resized_img = cv.resize(img, (500, 500))\n",
    "        asl_images[letter] = resized_img\n",
    "    else:\n",
    "        print(f\"The image for '{letter}' is not found at : {path}\")\n",
    "\n",
    "# a function to display each letter as an ASL gesture\n",
    "def display_asl_gesture(text):\n",
    "    for char in text:\n",
    "        # again, convert the space (' ') to '_' for the dictionary\n",
    "        char = '_' if char == ' ' else char.upper()\n",
    "\n",
    "        # get the pre-loaded and resized ASL image\n",
    "        img = asl_images.get(char)\n",
    "        if img is not None:\n",
    "            cv.imshow(f\"ASL Gesture for {char}\", img)\n",
    "            cv.waitKey(1000)  #set the display time for each image to 1 second\n",
    "            cv.destroyWindow(f\"ASL Gesture for {char}\")\n",
    "        else:\n",
    "            print(f\"ASL gesture for '{char}' is not available.\")\n",
    "\n",
    "# input text to be translated to ASL\n",
    "input_text = input(\"Enter text to translate to ASL gestures: \").strip()\n",
    "\n",
    "# call the display ASL function\n",
    "display_asl_gesture(input_text)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to install the following\n",
    "# pip install SpeechRecognition\n",
    "# pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courtesy of: https://pypi.org/project/SpeechRecognition/\n",
    "# and: https://stackoverflow.com/questions/62659602/how-can-i-use-speech-recognition-in-python3-in-the-mac-i-downloaded-but-pyaudio\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the speech recognizer\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text_to_asl():\n",
    "    print(\"Say something...\")\n",
    "\n",
    "    # Start capturing audio\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio_data = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize speech using Google's speech recognition\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"Recognized Text: {text}\")\n",
    "\n",
    "            # Call the ASL display function with the recognized text\n",
    "            display_asl_gesture(text)\n",
    "            \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech was unclear. Please try again.\")\n",
    "        except sr.RequestError:\n",
    "            print(\"Could not request results; check your internet connection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "Recognized Text: hello\n",
      "ASL gesture for 'H' is not available.\n",
      "ASL gesture for 'E' is not available.\n",
      "ASL gesture for 'L' is not available.\n",
      "ASL gesture for 'L' is not available.\n",
      "ASL gesture for 'O' is not available.\n"
     ]
    }
   ],
   "source": [
    "# Run the speech-to-text-to-ASL function\n",
    "speech_to_text_to_asl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation using google translator link:https://deep-translator.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 'aa', 'name': 'AFAR'},\n",
       " {'code': 'ab', 'name': 'ABKHAZIAN'},\n",
       " {'code': 'af', 'name': 'AFRIKAANS'},\n",
       " {'code': 'ak', 'name': 'AKAN'},\n",
       " {'code': 'am', 'name': 'AMHARIC'},\n",
       " {'code': 'ar', 'name': 'ARABIC'},\n",
       " {'code': 'as', 'name': 'ASSAMESE'},\n",
       " {'code': 'ay', 'name': 'AYMARA'},\n",
       " {'code': 'az', 'name': 'AZERBAIJANI'},\n",
       " {'code': 'ba', 'name': 'BASHKIR'},\n",
       " {'code': 'be', 'name': 'BELARUSIAN'},\n",
       " {'code': 'bg', 'name': 'BULGARIAN'},\n",
       " {'code': 'bh', 'name': 'BIHARI'},\n",
       " {'code': 'bi', 'name': 'BISLAMA'},\n",
       " {'code': 'bn', 'name': 'BENGALI'},\n",
       " {'code': 'bo', 'name': 'TIBETAN'},\n",
       " {'code': 'br', 'name': 'BRETON'},\n",
       " {'code': 'bs', 'name': 'BOSNIAN'},\n",
       " {'code': 'bug', 'name': 'BUGINESE'},\n",
       " {'code': 'ca', 'name': 'CATALAN'},\n",
       " {'code': 'ceb', 'name': 'CEBUANO'},\n",
       " {'code': 'chr', 'name': 'CHEROKEE'},\n",
       " {'code': 'co', 'name': 'CORSICAN'},\n",
       " {'code': 'crs', 'name': 'SESELWA'},\n",
       " {'code': 'cs', 'name': 'CZECH'},\n",
       " {'code': 'cy', 'name': 'WELSH'},\n",
       " {'code': 'da', 'name': 'DANISH'},\n",
       " {'code': 'de', 'name': 'GERMAN'},\n",
       " {'code': 'dv', 'name': 'DHIVEHI'},\n",
       " {'code': 'dz', 'name': 'DZONGKHA'},\n",
       " {'code': 'egy', 'name': 'EGYPTIAN'},\n",
       " {'code': 'el', 'name': 'GREEK'},\n",
       " {'code': 'en', 'name': 'ENGLISH'},\n",
       " {'code': 'eo', 'name': 'ESPERANTO'},\n",
       " {'code': 'es', 'name': 'SPANISH'},\n",
       " {'code': 'et', 'name': 'ESTONIAN'},\n",
       " {'code': 'eu', 'name': 'BASQUE'},\n",
       " {'code': 'fa', 'name': 'PERSIAN'},\n",
       " {'code': 'fi', 'name': 'FINNISH'},\n",
       " {'code': 'fj', 'name': 'FIJIAN'},\n",
       " {'code': 'fo', 'name': 'FAROESE'},\n",
       " {'code': 'fr', 'name': 'FRENCH'},\n",
       " {'code': 'fy', 'name': 'FRISIAN'},\n",
       " {'code': 'ga', 'name': 'IRISH'},\n",
       " {'code': 'gd', 'name': 'SCOTS_GAELIC'},\n",
       " {'code': 'gl', 'name': 'GALICIAN'},\n",
       " {'code': 'gn', 'name': 'GUARANI'},\n",
       " {'code': 'got', 'name': 'GOTHIC'},\n",
       " {'code': 'gu', 'name': 'GUJARATI'},\n",
       " {'code': 'gv', 'name': 'MANX'},\n",
       " {'code': 'ha', 'name': 'HAUSA'},\n",
       " {'code': 'haw', 'name': 'HAWAIIAN'},\n",
       " {'code': 'hi', 'name': 'HINDI'},\n",
       " {'code': 'hmn', 'name': 'HMONG'},\n",
       " {'code': 'hr', 'name': 'CROATIAN'},\n",
       " {'code': 'ht', 'name': 'HAITIAN_CREOLE'},\n",
       " {'code': 'hu', 'name': 'HUNGARIAN'},\n",
       " {'code': 'hy', 'name': 'ARMENIAN'},\n",
       " {'code': 'ia', 'name': 'INTERLINGUA'},\n",
       " {'code': 'id', 'name': 'INDONESIAN'},\n",
       " {'code': 'ie', 'name': 'INTERLINGUE'},\n",
       " {'code': 'ig', 'name': 'IGBO'},\n",
       " {'code': 'ik', 'name': 'INUPIAK'},\n",
       " {'code': 'is', 'name': 'ICELANDIC'},\n",
       " {'code': 'it', 'name': 'ITALIAN'},\n",
       " {'code': 'iu', 'name': 'INUKTITUT'},\n",
       " {'code': 'iw', 'name': 'HEBREW'},\n",
       " {'code': 'ja', 'name': 'JAPANESE'},\n",
       " {'code': 'jw', 'name': 'JAVANESE'},\n",
       " {'code': 'ka', 'name': 'GEORGIAN'},\n",
       " {'code': 'kha', 'name': 'KHASI'},\n",
       " {'code': 'kk', 'name': 'KAZAKH'},\n",
       " {'code': 'kl', 'name': 'GREENLANDIC'},\n",
       " {'code': 'km', 'name': 'KHMER'},\n",
       " {'code': 'kn', 'name': 'KANNADA'},\n",
       " {'code': 'ko', 'name': 'KOREAN'},\n",
       " {'code': 'ks', 'name': 'KASHMIRI'},\n",
       " {'code': 'ku', 'name': 'KURDISH'},\n",
       " {'code': 'ky', 'name': 'KYRGYZ'},\n",
       " {'code': 'la', 'name': 'LATIN'},\n",
       " {'code': 'lb', 'name': 'LUXEMBOURGISH'},\n",
       " {'code': 'lg', 'name': 'GANDA'},\n",
       " {'code': 'lif', 'name': 'LIMBU'},\n",
       " {'code': 'ln', 'name': 'LINGALA'},\n",
       " {'code': 'lo', 'name': 'LAOTHIAN'},\n",
       " {'code': 'lt', 'name': 'LITHUANIAN'},\n",
       " {'code': 'lv', 'name': 'LATVIAN'},\n",
       " {'code': 'mfe', 'name': 'MAURITIAN_CREOLE'},\n",
       " {'code': 'mg', 'name': 'MALAGASY'},\n",
       " {'code': 'mi', 'name': 'MAORI'},\n",
       " {'code': 'mk', 'name': 'MACEDONIAN'},\n",
       " {'code': 'ml', 'name': 'MALAYALAM'},\n",
       " {'code': 'mn', 'name': 'MONGOLIAN'},\n",
       " {'code': 'mr', 'name': 'MARATHI'},\n",
       " {'code': 'ms', 'name': 'MALAY'},\n",
       " {'code': 'mt', 'name': 'MALTESE'},\n",
       " {'code': 'my', 'name': 'BURMESE'},\n",
       " {'code': 'na', 'name': 'NAURU'},\n",
       " {'code': 'ne', 'name': 'NEPALI'},\n",
       " {'code': 'nl', 'name': 'DUTCH'},\n",
       " {'code': 'no', 'name': 'NORWEGIAN'},\n",
       " {'code': 'nr', 'name': 'NDEBELE'},\n",
       " {'code': 'nso', 'name': 'PEDI'},\n",
       " {'code': 'ny', 'name': 'NYANJA'},\n",
       " {'code': 'oc', 'name': 'OCCITAN'},\n",
       " {'code': 'om', 'name': 'OROMO'},\n",
       " {'code': 'or', 'name': 'ORIYA'},\n",
       " {'code': 'pa', 'name': 'PUNJABI'},\n",
       " {'code': 'pl', 'name': 'POLISH'},\n",
       " {'code': 'ps', 'name': 'PASHTO'},\n",
       " {'code': 'pt', 'name': 'PORTUGUESE'},\n",
       " {'code': 'qu', 'name': 'QUECHUA'},\n",
       " {'code': 'rm', 'name': 'RHAETO_ROMANCE'},\n",
       " {'code': 'rn', 'name': 'RUNDI'},\n",
       " {'code': 'ro', 'name': 'ROMANIAN'},\n",
       " {'code': 'ru', 'name': 'RUSSIAN'},\n",
       " {'code': 'rw', 'name': 'KINYARWANDA'},\n",
       " {'code': 'sa', 'name': 'SANSKRIT'},\n",
       " {'code': 'sco', 'name': 'SCOTS'},\n",
       " {'code': 'sd', 'name': 'SINDHI'},\n",
       " {'code': 'sg', 'name': 'SANGO'},\n",
       " {'code': 'si', 'name': 'SINHALESE'},\n",
       " {'code': 'sk', 'name': 'SLOVAK'},\n",
       " {'code': 'sl', 'name': 'SLOVENIAN'},\n",
       " {'code': 'sm', 'name': 'SAMOAN'},\n",
       " {'code': 'sn', 'name': 'SHONA'},\n",
       " {'code': 'so', 'name': 'SOMALI'},\n",
       " {'code': 'sq', 'name': 'ALBANIAN'},\n",
       " {'code': 'sr', 'name': 'SERBIAN'},\n",
       " {'code': 'ss', 'name': 'SISWANT'},\n",
       " {'code': 'st', 'name': 'SESOTHO'},\n",
       " {'code': 'su', 'name': 'SUNDANESE'},\n",
       " {'code': 'sv', 'name': 'SWEDISH'},\n",
       " {'code': 'sw', 'name': 'SWAHILI'},\n",
       " {'code': 'syr', 'name': 'SYRIAC'},\n",
       " {'code': 'ta', 'name': 'TAMIL'},\n",
       " {'code': 'te', 'name': 'TELUGU'},\n",
       " {'code': 'tg', 'name': 'TAJIK'},\n",
       " {'code': 'th', 'name': 'THAI'},\n",
       " {'code': 'ti', 'name': 'TIGRINYA'},\n",
       " {'code': 'tk', 'name': 'TURKMEN'},\n",
       " {'code': 'tl', 'name': 'TAGALOG'},\n",
       " {'code': 'tlh', 'name': 'KLINGON'},\n",
       " {'code': 'tn', 'name': 'TSWANA'},\n",
       " {'code': 'to', 'name': 'TONGA'},\n",
       " {'code': 'tr', 'name': 'TURKISH'},\n",
       " {'code': 'ts', 'name': 'TSONGA'},\n",
       " {'code': 'tt', 'name': 'TATAR'},\n",
       " {'code': 'ug', 'name': 'UIGHUR'},\n",
       " {'code': 'uk', 'name': 'UKRAINIAN'},\n",
       " {'code': 'ur', 'name': 'URDU'},\n",
       " {'code': 'uz', 'name': 'UZBEK'},\n",
       " {'code': 've', 'name': 'VENDA'},\n",
       " {'code': 'vi', 'name': 'VIETNAMESE'},\n",
       " {'code': 'vo', 'name': 'VOLAPUK'},\n",
       " {'code': 'war', 'name': 'WARAY_PHILIPPINES'},\n",
       " {'code': 'wo', 'name': 'WOLOF'},\n",
       " {'code': 'xh', 'name': 'XHOSA'},\n",
       " {'code': 'yi', 'name': 'YIDDISH'},\n",
       " {'code': 'yo', 'name': 'YORUBA'},\n",
       " {'code': 'za', 'name': 'ZHUANG'},\n",
       " {'code': 'zh', 'name': 'CHINESE_SIMPLIFIED'},\n",
       " {'code': 'zh-Hant', 'name': 'CHINESE_TRADITIONAL'},\n",
       " {'code': 'zu', 'name': 'ZULU'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "languages = requests.get(\"https://ws.detectlanguage.com/0.2/languages\")\n",
    "languages.json()  # displays the codes for various languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment allez-vous'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(text, target_language):\n",
    "    translated_text = GoogleTranslator(source=\"auto\", target=target_language).translate(\n",
    "        text\n",
    "    )\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "text_input = input(\"Enter text\")  # Replace with extracted text from the sign language\n",
    "target_lang = input(\"Enter target language\")  # replace with target language\n",
    "\n",
    "translated_text = translate(text_input, target_lang)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO No GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"small2.pt\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "# confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "live_text = \"\"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_WIDTH, 240)\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    # cv.imshow('frame', gray)\n",
    "\n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "\n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[\n",
    "        0\n",
    "    ].probs.top1conf  # Get confidence of the top-class prediction\n",
    "\n",
    "    # If the confidence of the sign is above the threshold\n",
    "    if top_confidence >= confidence_requirement:\n",
    "\n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter\n",
    "        else:\n",
    "\n",
    "            count = 1\n",
    "\n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(translator[top_class])\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if (translator[top_class] != \"DEL\") and (\n",
    "                translator[top_class] != \"NOTHING\"\n",
    "            ):\n",
    "\n",
    "                live_text = live_text + translator[top_class]\n",
    "\n",
    "            elif translator[top_class] == \"DEL\":\n",
    "\n",
    "                # https://stackoverflow.com/questions/15478127/remove-final-character-from-string\n",
    "                live_text = live_text[:-1]\n",
    "\n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "\n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0\n",
    "\n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "\n",
    "        noise_count = noise_count + 1\n",
    "\n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    # annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "\n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(\n",
    "        frame,\n",
    "        \"Text: \" + live_text,\n",
    "        (10, 460),\n",
    "        cv.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "        cv.LINE_4,\n",
    "    )\n",
    "\n",
    "    cv.imshow(\"Capture\", frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "        if DEBUG == True:\n",
    "\n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "\n",
    "            for i in range(len(captured_text)):\n",
    "\n",
    "                print(\n",
    "                    \"Translated text: \",\n",
    "                    captured_text[i],\n",
    "                    \" Confidence: \",\n",
    "                    captured_confidence[i].item(),\n",
    "                )\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "import ttkbootstrap as ttkb  # Modern mobile-like design\n",
    "from ttkbootstrap.constants import PRIMARY\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"small2.pt\")\n",
    "# model = YOLO(\"C:/Users/mbele/Desktop/capstone/Capstone-Project/old/best_v3_yolov11.pt\")\n",
    "DEBUG = True\n",
    "confidence_requirement = 0.90\n",
    "\n",
    "captured_text = []\n",
    "captured_confidence = []\n",
    "count = 0\n",
    "noise_count = 0\n",
    "last = None\n",
    "live_text = \"\"\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "\n",
    "# Initialize the Tkinter window using ttkbootstrap for modern styling\n",
    "window = ttkb.Window(themename=\"darkly\")  # Set a modern, mobile-like theme\n",
    "window.title(\"Sign Language Detection App\")\n",
    "window.geometry(\"480x800\")\n",
    "\n",
    "# Create a Label to display video feed\n",
    "video_frame = ttkb.Frame(window, padding=10)\n",
    "video_frame.pack(pady=10)\n",
    "\n",
    "video_label = ttkb.Label(video_frame)\n",
    "video_label.pack()\n",
    "\n",
    "# Create a ScrolledText field to display live translated text (expandable with scroll)\n",
    "translated_text_display = ScrolledText(\n",
    "    window, font=(\"Arial\", 18), height=5, wrap=tk.WORD, bg=\"#282828\", fg=\"#f1f1f1\"\n",
    ")\n",
    "translated_text_display.pack(padx=10, pady=10, fill=\"x\")\n",
    "\n",
    "# Function to update the video frame\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Set the desired FPS (e.g., 30 for 30 frames per second)\n",
    "# FPS = 10\n",
    "# frame_delay = int(1000 / FPS)  # Calculate delay in milliseconds\n",
    "\n",
    "\n",
    "def update_frame():\n",
    "    global last, count, noise_count, live_text\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "        top_class = results[0].probs.top1\n",
    "        top_confidence = results[0].probs.top1conf\n",
    "\n",
    "        if top_confidence >= confidence_requirement:\n",
    "            if top_class == last:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "\n",
    "            if count == 3:\n",
    "                if DEBUG:\n",
    "                    captured_text.append(translator[top_class])\n",
    "                    captured_confidence.append(top_confidence)\n",
    "\n",
    "                if translator[top_class] not in [\"DEL\", \"NOTHING\"]:\n",
    "                    live_text += translator[top_class]\n",
    "                elif translator[top_class] == \"DEL\":\n",
    "                    live_text = live_text[:-1]\n",
    "\n",
    "            last = top_class\n",
    "            noise_count = 0\n",
    "        else:\n",
    "            noise_count += 1\n",
    "\n",
    "        if noise_count == 3:\n",
    "            count = 0\n",
    "            noise_count = 0\n",
    "\n",
    "        # Convert frame to ImageTk for Tkinter display\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        img_tk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        video_label.imgtk = img_tk\n",
    "        video_label.configure(image=img_tk)\n",
    "\n",
    "        # Update the live text display (scrollable and expandable)\n",
    "        translated_text_display.delete(1.0, tk.END)  # Clear current text\n",
    "        translated_text_display.insert(tk.END, live_text)  # Insert updated live text\n",
    "        translated_text_display.see(tk.END)  # Auto-scroll to the end\n",
    "\n",
    "    # Continue updating the frame with the specified frame rate\n",
    "    window.after(int(1), update_frame)  # Use the calculated frame delay\n",
    "\n",
    "\n",
    "# Create Start and Quit buttons with modern mobile styling\n",
    "def start_feed():\n",
    "    update_frame()\n",
    "\n",
    "\n",
    "def quit_app():\n",
    "    if DEBUG:\n",
    "        print(\"\\n\\nCaptured Text:\")\n",
    "        for i in range(len(captured_text)):\n",
    "            print(\n",
    "                \"Translated text: \",\n",
    "                captured_text[i],\n",
    "                \" Confidence: \",\n",
    "                captured_confidence[i].item(),\n",
    "            )\n",
    "    cap.release()\n",
    "    window.quit()\n",
    "\n",
    "\n",
    "button_frame = ttkb.Frame(window, padding=10)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "start_button = ttkb.Button(\n",
    "    button_frame, text=\"Start Detection\", command=start_feed, bootstyle=PRIMARY\n",
    ")\n",
    "start_button.pack(side=\"left\", padx=10)\n",
    "\n",
    "quit_button = ttkb.Button(\n",
    "    button_frame, text=\"Quit\", command=quit_app, bootstyle=PRIMARY\n",
    ")\n",
    "quit_button.pack(side=\"right\", padx=10)\n",
    "\n",
    "# Start the app loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO with GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Captured Text:\n"
     ]
    }
   ],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"small2.pt\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "# confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"CONFIRM\",\n",
    "    \"DEL\",\n",
    "    \"NOTHING\",\n",
    "    \"_\",\n",
    "]\n",
    "live_text = \"\"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_WIDTH, 500)\n",
    "    # ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT, 500)\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    # cv.imshow('frame', gray)\n",
    "\n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "\n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[\n",
    "        0\n",
    "    ].probs.top1conf  # Get confidence of the top-class prediction\n",
    "\n",
    "    # If the confidence of the sign is above the threshold\n",
    "    if top_confidence >= confidence_requirement:\n",
    "\n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter\n",
    "        else:\n",
    "\n",
    "            count = 1\n",
    "\n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(translator[top_class])\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if (translator[top_class] != \"DEL\") and (\n",
    "                translator[top_class] != \"NOTHING\"\n",
    "            ):\n",
    "\n",
    "                live_text = live_text + translator[top_class]\n",
    "\n",
    "            elif translator[top_class] == \"DEL\":\n",
    "\n",
    "                # https://stackoverflow.com/questions/15478127/remove-final-character-from-string\n",
    "                live_text = live_text[:-1]\n",
    "\n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "\n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0\n",
    "\n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "\n",
    "        noise_count = noise_count + 1\n",
    "\n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    # annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "\n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(\n",
    "        frame,\n",
    "        \"Text: \" + live_text,\n",
    "        (10, 460),\n",
    "        cv.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 255),\n",
    "        2,\n",
    "        cv.LINE_4,\n",
    "    )\n",
    "\n",
    "    cv.imshow(\"Capture\", frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord(\"q\"):\n",
    "\n",
    "        if DEBUG == True:\n",
    "\n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "\n",
    "            for i in range(len(captured_text)):\n",
    "\n",
    "                print(\n",
    "                    \"Translated text: \",\n",
    "                    captured_text[i],\n",
    "                    \" Confidence: \",\n",
    "                    captured_confidence[i].item(),\n",
    "                )\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired FPS (e.g., 30 for 30 frames per second)\n",
    "FPS = 10\n",
    "frame_delay = int(1000 / FPS)  # Calculate delay in milliseconds\n",
    "\n",
    "\n",
    "def update_frame():\n",
    "    global last, count, noise_count, live_text\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "        top_class = results[0].probs.top1\n",
    "        top_confidence = results[0].probs.top1conf\n",
    "\n",
    "        if top_confidence >= confidence_requirement:\n",
    "            if top_class == last:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "\n",
    "            if count == 3:\n",
    "                if DEBUG:\n",
    "                    captured_text.append(translator[top_class])\n",
    "                    captured_confidence.append(top_confidence)\n",
    "\n",
    "                if translator[top_class] not in [\"DEL\", \"NOTHING\"]:\n",
    "                    live_text += translator[top_class]\n",
    "                elif translator[top_class] == \"DEL\":\n",
    "                    live_text = live_text[:-1]\n",
    "\n",
    "            last = top_class\n",
    "            noise_count = 0\n",
    "        else:\n",
    "            noise_count += 1\n",
    "\n",
    "        if noise_count == 3:\n",
    "            count = 0\n",
    "            noise_count = 0\n",
    "\n",
    "        # Convert frame to ImageTk for Tkinter display\n",
    "        img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        img_tk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        video_label.imgtk = img_tk\n",
    "        video_label.configure(image=img_tk)\n",
    "\n",
    "        # Update the live text display (scrollable and expandable)\n",
    "        translated_text_display.delete(1.0, tk.END)  # Clear current text\n",
    "        translated_text_display.insert(tk.END, live_text)  # Insert updated live text\n",
    "        translated_text_display.see(tk.END)  # Auto-scroll to the end\n",
    "\n",
    "    # Continue updating the frame with the specified frame rate\n",
    "    window.after(frame_delay, update_frame)  # Use the calculated frame delay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
