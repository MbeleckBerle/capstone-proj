{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from spellchecker import SpellChecker\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words"
      ],
      "metadata": {
        "id": "gGzFoYTUrDtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ccdf97-beb5-4d56-c867-8e4a01c5e770"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLO INPUTS SIMULATION"
      ],
      "metadata": {
        "id": "uV0HEGnqsMTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO OUTPUTs\n",
        "\n",
        "# CONFUSING WORDS ANALYSIS\n",
        "\n",
        "# Many issues come from a poor diet. For example, sweets can Xffect your health.\n",
        "# Effect x Affect example\n",
        "\n",
        "input_1 = \"MANY ISSUES COME FROM A POOR DIET FOR EXAMPLE SWEETS CAN XFFECT YOUR HEALTH\"\n",
        "\n",
        "# The PRINCIPLE of the school is very strict.\n",
        "# principle x principal example\n",
        "\n",
        "input_2 = \"THE PRINCIPEL OF THE SCHOOL IS VERY STRICT\"\n",
        "\n",
        "# You should know how to SITE your sources\n",
        "# cite x site x sight example\n",
        "\n",
        "input_3 = \"YOU NEED TO SITE YOUR REFERENCES CORRECTLY IN YOUR RESEARCH PAPER.\"\n",
        "\n",
        "# After a big family dinner, everyone was excited to see what delicious DESERT would be served.\n",
        "# dessert x desert example\n",
        "\n",
        "input_4 = \"AFTER A BIG FAMILY DINNER EVERYONE WAS EXCITED TO SEE WHAT DELICIOUS DESERT WOULD BE SERVED\"\n",
        "\n",
        "# FILL THE WORD ANALYSIS\n",
        "\n",
        "# Context is important for understanding the meanings.\n",
        "# Example with a missing letter in \"understanding\"\n",
        "\n",
        "input_5 = \"CONTEXT IS IMPORTANT FOR UNDERSANDING THE MEANINGS\"\n",
        "\n",
        "# Traveling exposes you to new cultures and experiences.\n",
        "# Example with a missing letter in \"cultures.\"\n",
        "\n",
        "input_6 = \"TRAVELING EXPOSES YOU TO NEW CULTURS AND EXPERIENCES\"\n",
        "\n",
        "# Technology bridges gaps between distant communities.\n",
        "# Example with missing letters in \"communities.\"\n",
        "\n",
        "input_7 = \"TECHNOLOGY BRIDGES GAPS BETWEEN DISTANT COMUNITIES\"\n",
        "\n",
        "# COMMON WORDS MISPELLED\n",
        "\n",
        "# Reading regularly can improve cognitive skills.\n",
        "# Misspelled word: \"cognitive\" -> \"cognitiva\"\n",
        "\n",
        "input_8 = \"READING REGULARLY CAN IMPROVE COGNITIVA SKILLS\"\n",
        "\n",
        "# The experiment showed good results.\n",
        "# Misspelled word: \"experiment\" -> \"experilent\"\n",
        "\n",
        "input_9 = \"THE EXPERILENT SHOWED GOOD RESULTS\"\n",
        "\n",
        "# Physical activity boosts energy levels.\n",
        "# Misspelled word: \"activity\" -> \"aktivwty\"\n",
        "\n",
        "input_10 = \"PHYSICAL AKTIVWTY BOOSTS ENERGY LEVELS\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OiXX89I7kY_x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Homophones dictionary\n",
        "homophones_dict = {\n",
        "    \"accept\": [\"except\"],\n",
        "    \"except\": [\"accept\"],\n",
        "    \"access\": [\"excess\"],\n",
        "    \"excess\": [\"access\"],\n",
        "    \"affect\": [\"effect\"],\n",
        "    \"effect\": [\"affect\"],\n",
        "    \"aisle\": [\"isle\"],\n",
        "    \"isle\": [\"aisle\"],\n",
        "    \"allowed\": [\"aloud\"],\n",
        "    \"aloud\": [\"allowed\"],\n",
        "    \"altar\": [\"alter\"],\n",
        "    \"alter\": [\"altar\"],\n",
        "    \"ant\": [\"aunt\"],\n",
        "    \"aunt\": [\"ant\"],\n",
        "    \"arc\": [\"ark\"],\n",
        "    \"ark\": [\"arc\"],\n",
        "    \"ate\": [\"eight\"],\n",
        "    \"eight\": [\"ate\"],\n",
        "    \"ball\": [\"bawl\"],\n",
        "    \"bawl\": [\"ball\"],\n",
        "    \"bare\": [\"bear\"],\n",
        "    \"bear\": [\"bare\"],\n",
        "    \"base\": [\"bass\"],\n",
        "    \"bass\": [\"base\"],\n",
        "    \"be\": [\"bee\"],\n",
        "    \"bee\": [\"be\"],\n",
        "    \"beach\": [\"beech\"],\n",
        "    \"beech\": [\"beach\"],\n",
        "    \"beat\": [\"beet\"],\n",
        "    \"beet\": [\"beat\"],\n",
        "    \"been\": [\"bin\"],\n",
        "    \"bin\": [\"been\"],\n",
        "    \"berth\": [\"birth\"],\n",
        "    \"birth\": [\"berth\"],\n",
        "    \"billed\": [\"build\"],\n",
        "    \"build\": [\"billed\"],\n",
        "    \"blew\": [\"blue\"],\n",
        "    \"blue\": [\"blew\"],\n",
        "    \"board\": [\"bored\"],\n",
        "    \"bored\": [\"board\"],\n",
        "    \"brake\": [\"break\"],\n",
        "    \"break\": [\"brake\"],\n",
        "    \"bread\": [\"bred\"],\n",
        "    \"bred\": [\"bread\"],\n",
        "    \"buy\": [\"by\", \"bye\"],\n",
        "    \"by\": [\"buy\", \"bye\"],\n",
        "    \"bye\": [\"buy\", \"by\"],\n",
        "    \"cache\": [\"cash\"],\n",
        "    \"cash\": [\"cache\"],\n",
        "    \"cell\": [\"sell\"],\n",
        "    \"sell\": [\"cell\"],\n",
        "    \"cent\": [\"scent\", \"sent\"],\n",
        "    \"scent\": [\"cent\", \"sent\"],\n",
        "    \"sent\": [\"cent\", \"scent\"],\n",
        "    \"cite\": [\"site\", \"sight\"],\n",
        "    \"site\": [\"cite\", \"sight\"],\n",
        "    \"sight\": [\"cite\", \"site\"],\n",
        "    \"clause\": [\"claws\"],\n",
        "    \"claws\": [\"clause\"],\n",
        "    \"coarse\": [\"course\"],\n",
        "    \"course\": [\"coarse\"],\n",
        "    \"creak\": [\"creek\"],\n",
        "    \"creek\": [\"creak\"],\n",
        "    \"dear\": [\"deer\"],\n",
        "    \"deer\": [\"dear\"],\n",
        "    \"desert\": [\"dessert\"],\n",
        "    \"dessert\": [\"desert\"],\n",
        "    \"die\": [\"dye\"],\n",
        "    \"dye\": [\"die\"],\n",
        "    \"doe\": [\"dough\"],\n",
        "    \"dough\": [\"doe\"],\n",
        "    \"dual\": [\"duel\"],\n",
        "    \"duel\": [\"dual\"],\n",
        "    \"earn\": [\"urn\"],\n",
        "    \"urn\": [\"earn\"],\n",
        "    \"fair\": [\"fare\"],\n",
        "    \"fare\": [\"fair\"],\n",
        "    \"fir\": [\"fur\"],\n",
        "    \"fur\": [\"fir\"],\n",
        "    \"flair\": [\"flare\"],\n",
        "    \"flare\": [\"flair\"],\n",
        "    \"flour\": [\"flower\"],\n",
        "    \"flower\": [\"flour\"],\n",
        "    \"for\": [\"four\", \"fore\"],\n",
        "    \"four\": [\"for\", \"fore\"],\n",
        "    \"fore\": [\"for\", \"four\"],\n",
        "    \"forth\": [\"fourth\"],\n",
        "    \"fourth\": [\"forth\"],\n",
        "    \"foul\": [\"fowl\"],\n",
        "    \"fowl\": [\"foul\"],\n",
        "    \"gait\": [\"gate\"],\n",
        "    \"gate\": [\"gait\"],\n",
        "    \"gorilla\": [\"guerrilla\"],\n",
        "    \"guerrilla\": [\"gorilla\"],\n",
        "    \"hair\": [\"hare\"],\n",
        "    \"hare\": [\"hair\"],\n",
        "    \"heal\": [\"heel\"],\n",
        "    \"heel\": [\"heal\"],\n",
        "    \"hear\": [\"here\"],\n",
        "    \"here\": [\"hear\"],\n",
        "    \"hi\": [\"high\"],\n",
        "    \"high\": [\"hi\"],\n",
        "    \"hole\": [\"whole\"],\n",
        "    \"whole\": [\"hole\"],\n",
        "    \"hour\": [\"our\"],\n",
        "    \"our\": [\"hour\"],\n",
        "    \"idle\": [\"idol\"],\n",
        "    \"idol\": [\"idle\"],\n",
        "    \"knight\": [\"night\"],\n",
        "    \"night\": [\"knight\"],\n",
        "    \"knot\": [\"not\"],\n",
        "    \"not\": [\"knot\"],\n",
        "    \"know\": [\"no\"],\n",
        "    \"no\": [\"know\"],\n",
        "    \"leak\": [\"leek\"],\n",
        "    \"leek\": [\"leak\"],\n",
        "    \"lessen\": [\"lesson\"],\n",
        "    \"lesson\": [\"lessen\"],\n",
        "    \"made\": [\"maid\"],\n",
        "    \"maid\": [\"made\"],\n",
        "    \"mail\": [\"male\"],\n",
        "    \"male\": [\"mail\"],\n",
        "    \"meat\": [\"meet\"],\n",
        "    \"meet\": [\"meat\"],\n",
        "    \"moose\": [\"mousse\"],\n",
        "    \"mousse\": [\"moose\"],\n",
        "    \"morning\": [\"mourning\"],\n",
        "    \"mourning\": [\"morning\"],\n",
        "    \"none\": [\"nun\"],\n",
        "    \"nun\": [\"none\"],\n",
        "    \"oar\": [\"ore\"],\n",
        "    \"ore\": [\"oar\"],\n",
        "    \"one\": [\"won\"],\n",
        "    \"won\": [\"one\"],\n",
        "    \"pail\": [\"pale\"],\n",
        "    \"pale\": [\"pail\"],\n",
        "    \"pair\": [\"pare\", \"pear\"],\n",
        "    \"pare\": [\"pair\", \"pear\"],\n",
        "    \"pear\": [\"pair\", \"pare\"],\n",
        "    \"peace\": [\"piece\"],\n",
        "    \"piece\": [\"peace\"],\n",
        "    \"plain\": [\"plane\"],\n",
        "    \"plane\": [\"plain\"],\n",
        "    \"principal\": [\"principle\"],\n",
        "    \"principle\": [\"principal\"],\n",
        "    \"profit\": [\"prophet\"],\n",
        "    \"prophet\": [\"profit\"],\n",
        "    \"rain\": [\"reign\", \"rein\"],\n",
        "    \"reign\": [\"rain\", \"rein\"],\n",
        "    \"rein\": [\"rain\", \"reign\"],\n",
        "    \"raise\": [\"rays\", \"raze\"],\n",
        "    \"rays\": [\"raise\", \"raze\"],\n",
        "    \"raze\": [\"raise\", \"rays\"],\n",
        "    \"right\": [\"write\", \"rite\"],\n",
        "    \"write\": [\"right\", \"rite\"],\n",
        "    \"rite\": [\"right\", \"write\"],\n",
        "    \"road\": [\"rode\"],\n",
        "    \"rode\": [\"road\"],\n",
        "    \"root\": [\"route\"],\n",
        "    \"route\": [\"root\"],\n",
        "    \"sea\": [\"see\"],\n",
        "    \"see\": [\"sea\"],\n",
        "    \"seam\": [\"seem\"],\n",
        "    \"seem\": [\"seam\"],\n",
        "    \"sew\": [\"so\", \"sow\"],\n",
        "    \"so\": [\"sew\", \"sow\"],\n",
        "    \"sow\": [\"sew\", \"so\"],\n",
        "    \"sole\": [\"soul\"],\n",
        "    \"soul\": [\"sole\"],\n",
        "    \"some\": [\"sum\"],\n",
        "    \"sum\": [\"some\"],\n",
        "    \"son\": [\"sun\"],\n",
        "    \"sun\": [\"son\"],\n",
        "    \"stair\": [\"stare\"],\n",
        "    \"stare\": [\"stair\"],\n",
        "    \"steal\": [\"steel\"],\n",
        "    \"steel\": [\"steal\"],\n",
        "    \"suite\": [\"sweet\"],\n",
        "    \"sweet\": [\"suite\"],\n",
        "    \"tail\": [\"tale\"],\n",
        "    \"tale\": [\"tail\"],\n",
        "    \"their\": [\"there\", \"they're\"],\n",
        "    \"there\": [\"their\", \"they're\"],\n",
        "    \"to\": [\"two\", \"too\"],\n",
        "    \"two\": [\"to\", \"too\"],\n",
        "    \"too\": [\"to\", \"two\"],\n",
        "    \"vain\": [\"vein\", \"vane\"],\n",
        "    \"vein\": [\"vain\", \"vane\"],\n",
        "    \"vane\": [\"vain\", \"vein\"],\n",
        "    \"waist\": [\"waste\"],\n",
        "    \"waste\": [\"waist\"],\n",
        "    \"wait\": [\"weight\"],\n",
        "    \"weight\": [\"wait\"],\n",
        "    \"way\": [\"weigh\"],\n",
        "    \"weigh\": [\"way\"],\n",
        "    \"weak\": [\"week\"],\n",
        "    \"week\": [\"weak\"],\n",
        "    \"weather\": [\"whether\"],\n",
        "    \"whether\": [\"weather\"],\n",
        "    \"which\": [\"witch\"],\n",
        "    \"witch\": [\"which\"],\n",
        "    \"wood\": [\"would\"],\n",
        "    \"would\": [\"wood\"],\n",
        "    \"yew\": [\"you\", \"ewe\"],\n",
        "    \"you\": [\"yew\", \"ewe\"],\n",
        "    \"ewe\": [\"yew\", \"you\"],\n",
        "    \"your\": [\"you're\"],\n",
        "    \"you're\": [\"your\"],\n",
        "}"
      ],
      "metadata": {
        "id": "ZUQYWrT_34qD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting to lowercase"
      ],
      "metadata": {
        "id": "4_JufSBYwJaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert all characters in a captured sequence to lowercase and join them into a single string\n",
        "def convert_to_lowercase_and_join(captured_sequence):\n",
        "    return ''.join([char.lower() if char.isalpha() else char for char in captured_sequence])\n",
        "\n",
        "\n",
        "# Convert each input sequence\n",
        "\n",
        "input_1 = convert_to_lowercase_and_join(input_1)\n",
        "input_2 = convert_to_lowercase_and_join(input_2)\n",
        "input_3 = convert_to_lowercase_and_join(input_3)\n",
        "input_4 = convert_to_lowercase_and_join(input_4)\n",
        "input_5 = convert_to_lowercase_and_join(input_5)\n",
        "input_6 = convert_to_lowercase_and_join(input_6)\n",
        "input_7 = convert_to_lowercase_and_join(input_7)\n",
        "input_8 = convert_to_lowercase_and_join(input_8)\n",
        "input_9 = convert_to_lowercase_and_join(input_9)\n",
        "input_10 = convert_to_lowercase_and_join(input_10)\n",
        "\n",
        "# Display the results\n",
        "\n",
        "converted_inputs = [input_1, input_2, input_3, input_4, input_5, input_6, input_7, input_8, input_9, input_10]\n",
        "\n",
        "for i, converted in enumerate(converted_inputs, 1):\n",
        "    print(f\"Converted Input {i}:\", \"\".join(converted))\n"
      ],
      "metadata": {
        "id": "L2LOxOqQwIJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bf94b6-b6e5-4711-faf8-217b0c23ff5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted Input 1: many issues come from a poor diet for example sweets can xffect your health\n",
            "Converted Input 2: the principel of the school is very strict\n",
            "Converted Input 3: you need to site your references correctly in your research paper.\n",
            "Converted Input 4: after a big family dinner everyone was excited to see what delicious desert would be served\n",
            "Converted Input 5: context is important for undersanding the meanings\n",
            "Converted Input 6: traveling exposes you to new culturs and experiences\n",
            "Converted Input 7: technology bridges gaps between distant comunities\n",
            "Converted Input 8: reading regularly can improve cognitiva skills\n",
            "Converted Input 9: the experilent showed good results\n",
            "Converted Input 10: physical aktivwty boosts energy levels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spell checker"
      ],
      "metadata": {
        "id": "LtpowAIds4yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load word list dataset\n",
        "word_list = set(words.words())\n",
        "spell = SpellChecker()\n",
        "\n",
        "# USED THIS VARIABLE FOR TEST!!\n",
        "\n",
        "captured_sequences = input_1  # Replace with the specific input variable you want to test\n",
        "\n",
        "# Helper function to check if a word is valid\n",
        "def is_valid_word(word):\n",
        "    return word in word_list  # No need for .lower() since captured_sequences is lowercase\n",
        "\n",
        "# Process the captured sequence\n",
        "sentence = \"\"\n",
        "current_word = \"\"\n",
        "\n",
        "for letter in captured_sequences:\n",
        "    if letter == ' ':\n",
        "        # Check and correct the current word, then reset\n",
        "        if current_word:\n",
        "            if not is_valid_word(current_word):\n",
        "                corrected_word = spell.correction(current_word)\n",
        "                sentence += corrected_word + \" \"  # Add corrected word with a space\n",
        "                print(f\"'{current_word}' is not a valid word. Corrected to '{corrected_word}'.\")\n",
        "            else:\n",
        "                sentence += current_word + \" \"  # Add space after the valid word\n",
        "            current_word = \"\"\n",
        "    else:\n",
        "        current_word += letter\n",
        "\n",
        "# Check and correct the last word (if any)\n",
        "if current_word:\n",
        "    if not is_valid_word(current_word):\n",
        "        corrected_word = spell.correction(current_word)\n",
        "        sentence += corrected_word  # Add corrected word without additional lowercase transformation\n",
        "        print(f\"'{current_word}' is not a valid word. Corrected to '{corrected_word}'.\")\n",
        "    else:\n",
        "        sentence += current_word\n",
        "\n",
        "print(\"\\nCaptured text:\", captured_sequences)\n",
        "print(\"\\nFinal Sentence:\", sentence.strip())\n"
      ],
      "metadata": {
        "id": "4BU9zgVpzI4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22721e43-70ee-4d9c-b81a-fd1c25d5c358"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'issues' is not a valid word. Corrected to 'issues'.\n",
            "'sweets' is not a valid word. Corrected to 'sweets'.\n",
            "'xffect' is not a valid word. Corrected to 'effect'.\n",
            "\n",
            "Captured text: many issues come from a poor diet for example sweets can xffect your health\n",
            "\n",
            "Final Sentence: many issues come from a poor diet for example sweets can effect your health\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correcting with Context"
      ],
      "metadata": {
        "id": "ILw3r-62tZNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Datamuse API"
      ],
      "metadata": {
        "id": "rVlpDqdRm0Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spell checker and fill-mask pipeline\n",
        "spell = SpellChecker()\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# Function to get homophones from Datamuse API\n",
        "def get_homophones(word):\n",
        "    response = requests.get(f\"https://api.datamuse.com/words?rel_hom={word}\")\n",
        "    homophones = [entry['word'] for entry in response.json()]\n",
        "    return homophones\n",
        "\n",
        "# Function to check and correct words\n",
        "def correct_sentence(sentence):\n",
        "    words = sentence.split()\n",
        "    corrected_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        # Check spelling\n",
        "        if word.lower() in word_list:  # Assuming word_list is defined with valid words\n",
        "            corrected_sentence.append(word)\n",
        "        else:\n",
        "            # Suggest correction\n",
        "            corrected_word = spell.correction(word)\n",
        "            corrected_sentence.append(corrected_word)\n",
        "\n",
        "            # Get homophones\n",
        "            homophones = get_homophones(corrected_word)\n",
        "            if homophones:\n",
        "                print(f\"Potentially confusing words for '{corrected_word}': {homophones}\")\n",
        "\n",
        "                # Replace word with a mask for BERT\n",
        "                masked_sentence = sentence.replace(word, \"[MASK]\")\n",
        "\n",
        "                # Apply BERT to predict the best word\n",
        "                predictions = fill_mask(masked_sentence)\n",
        "\n",
        "                print(\"Predictions: \", predictions)\n",
        "\n",
        "                # Replace the mask with the best prediction\n",
        "                if predictions:\n",
        "                    # Make a list containing the corrected word and homophones\n",
        "                    c = [corrected_word] + homophones\n",
        "\n",
        "                    # Print them for testing\n",
        "                    print(\"words: \", c)\n",
        "\n",
        "                    # Look at predictions up to the available number\n",
        "                    for i in range(min(50, len(predictions))):\n",
        "\n",
        "                        # If the prediction is in c (correct word + homophones), use it\n",
        "                        if predictions[i]['token_str'] in c:\n",
        "                            # Choose this as the correct word\n",
        "                            corrected_sentence[-1] = predictions[i]['token_str']  # Replace last word with prediction\n",
        "\n",
        "                            # Exit the loop\n",
        "                            break\n",
        "\n",
        "    return ' '.join(corrected_sentence)\n",
        "\n",
        "# Example usage\n",
        "input_sentence = sentence\n",
        "corrected = correct_sentence(input_sentence)\n",
        "print(\"Corrected Sentence:\", corrected)\n"
      ],
      "metadata": {
        "id": "eINHQ3Nf6vnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wW_ktCm0Qyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hqZruGP2y_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pyx-SneP2zCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the dictionary created manually"
      ],
      "metadata": {
        "id": "JPDncAyembbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spell checker and fill-mask pipeline\n",
        "spell = SpellChecker()\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# Helper function to get homophones from the dictionary\n",
        "def get_homophones(word):\n",
        "    return homophones_dict.get(word.lower(), [])\n",
        "\n",
        "# Function to mask only words found in the homophones dictionary\n",
        "def mask_homophones(sentence):\n",
        "    words = sentence.split()\n",
        "    masked_sentence = []\n",
        "\n",
        "    # Replace words in the homophones dictionary with `[MASK]`\n",
        "    for word in words:\n",
        "        lower_word = word.lower()\n",
        "        if lower_word in homophones_dict:\n",
        "            masked_sentence.append(\"[MASK]\")  # Mask only homophones\n",
        "        else:\n",
        "            masked_sentence.append(word)\n",
        "\n",
        "    return ' '.join(masked_sentence)\n",
        "\n",
        "# Main function to process the sentence\n",
        "def correct_sentence(sentence):\n",
        "    # Create a masked version of the sentence\n",
        "    masked_sentence = mask_homophones(sentence)\n",
        "\n",
        "    # Get BERT predictions for each masked word\n",
        "    predictions = fill_mask(masked_sentence)\n",
        "\n",
        "    # Determine structure of predictions for compatibility\n",
        "    if isinstance(predictions[0], dict):\n",
        "        # Each prediction is a dictionary with 'token_str' as a key\n",
        "        get_token = lambda prediction: prediction['token_str']\n",
        "    elif isinstance(predictions[0], list) and isinstance(predictions[0][0], dict):\n",
        "        # Each prediction is a list of dictionaries, access the first element\n",
        "        get_token = lambda prediction: prediction[0]['token_str']\n",
        "    else:\n",
        "        # Unexpected format; fallback to return original sentence in uppercase\n",
        "        print(\"Unexpected prediction format:\", predictions)\n",
        "        return sentence.upper()\n",
        "\n",
        "    # Reconstruct the sentence with BERT’s predicted replacements\n",
        "    words = sentence.split()\n",
        "    corrected_sentence = []\n",
        "    prediction_index = 0  # Keep track of predictions for each `[MASK]`\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        # Only replace masked words\n",
        "        if word.lower() in homophones_dict:\n",
        "            # Get the predicted word using the determined `get_token` function\n",
        "            predicted_word = get_token(predictions[prediction_index]).upper()\n",
        "            prediction_index += 1\n",
        "\n",
        "            # Check if the predicted word is in the homophones list for this word\n",
        "            if predicted_word.lower() in homophones_dict[word.lower()]:\n",
        "                corrected_sentence.append(predicted_word)\n",
        "            else:\n",
        "                # If BERT prediction isn’t a homophone, keep the original word\n",
        "                corrected_sentence.append(word.upper())\n",
        "        else:\n",
        "            # Keep original words that aren’t in the homophones dictionary\n",
        "            corrected_sentence.append(word.upper())\n",
        "\n",
        "    return ' '.join(corrected_sentence)\n",
        "\n",
        "# Example usage\n",
        "input_sentence = sentence\n",
        "corrected = correct_sentence(input_sentence)\n",
        "print(\"Corrected Sentence:\", corrected)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0TNSlKl42zEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7917831-7bae-436e-e2f5-31e7da1cb2a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Sentence: MANY ISSUES COME FROM A POOR DIET FOR EXAMPLE SWEETS CAN AFFECT YOUR HEALTH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jD95QA3K20Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
