{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"best_v3_yolov11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cpu\n",
      "0.20.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Captured Text:\n",
      "Translated text:  U  Confidence:  0.9990386962890625\n",
      "Translated text:  H  Confidence:  0.9212673306465149\n",
      "Speaking:  UH\n",
      "Replaying:  UH\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "# This code started as a direct copy from opencv\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "\n",
    "#pyttsx3 courtesy of: https://pypi.org/project/pyttsx3/\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pyttsx3\n",
    "\n",
    "# Info about YOLO import and loading the Yolo Model\n",
    "# https://docs.ultralytics.com/tasks/classify/#train\n",
    "\n",
    "# Added this to import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# If DEBUG is True we will output all the detected signs and their confidence\n",
    "DEBUG = True\n",
    "\n",
    "# Initialize pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# set speech speed rate\n",
    "default_rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', int(default_rate * 0.75))  # set the speed rate to 75%\n",
    "\n",
    "# select voice gender\n",
    "def select_voice(engine, gender='m'):\n",
    "    voices = engine.getProperty('voices')\n",
    "    if gender == 'f':\n",
    "        engine.setProperty('voice', voices[1].id)  #1 for female\n",
    "    else:\n",
    "        engine.setProperty('voice', voices[0].id)  #0 for male\n",
    "\n",
    "# prompt to select gender\n",
    "gender = input(\"Select voice (m: male / f: female) : \").strip().lower()\n",
    "select_voice(engine, gender)\n",
    "\n",
    "# Load the YOLOv8 model, this is loading our custom trained weights for our model.\n",
    "model = YOLO(\"best_v3_yolov11.pt\")\n",
    " \n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Stored captured text\n",
    "captured_text = []\n",
    "\n",
    "# Stores captured confidence\n",
    "captured_confidence = []\n",
    "\n",
    "# Set a threshold for the sign to register\n",
    "confidence_requirement = 0.90\n",
    "#confidence_requirement = 0.30\n",
    "\n",
    "# Counts the number of consecutive significant signs\n",
    "count = 0\n",
    "\n",
    "# Counts the number of consecutive insignificant signs\n",
    "noise_count = 0\n",
    "\n",
    "# Keeps track of the last sign\n",
    "last = None\n",
    "\n",
    "translator = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"DEL\",\"NOTHING\",\" \"]\n",
    "live_text = \"Text: \"\n",
    "\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    " \n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    \n",
    "    # These two lines are found here https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    # They reduce the size of the video\n",
    "    \n",
    "    #ret = cap.set(cv.CAP_PROP_FRAME_WIDTH,240)\n",
    "    #ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT,240)\n",
    "    \n",
    "    # Our operations on the frame come here\n",
    "    #gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Display the resulting frame\n",
    "    #cv.imshow('frame', gray)\n",
    "    \n",
    "    # If you scroll to the very bottom of this link\n",
    "    # https://docs.ultralytics.com/modes/predict/#thread-safe-inference\n",
    "    # You will find the next 3 lines of code which I took from their and applied to this similar example\n",
    "    \n",
    "    # Info about reducing output from https://github.com/ultralytics/ultralytics/issues/1896\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame, verbose=False)\n",
    "    top_class = results[0].probs.top1\n",
    "    top_confidence = results[0].probs.top1conf  # Get confidence of the top-class prediction\n",
    "    \n",
    "    # If the confidence of the sign is above the threshold \n",
    "    if top_confidence >= confidence_requirement:\n",
    "        \n",
    "        # If the top_class is the last class\n",
    "        # hence it is consecutive increase count by 1.\n",
    "        if top_class == last:\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        # If the top_class is not the last class\n",
    "        # it is  a new class restart counter \n",
    "        else:\n",
    "    \n",
    "            count = 1\n",
    "            \n",
    "        # If there are 3 consecutive significant signs track it\n",
    "        if count == 3:\n",
    "            sign = translator[top_class]\n",
    "\n",
    "            if DEBUG == True:\n",
    "                captured_text.append(sign)\n",
    "                captured_confidence.append(top_confidence)\n",
    "\n",
    "            if sign == \"SPACE\":\n",
    "                captured_text.append(\"_\")  # Store \"_\" for space in captured_text\n",
    "                live_text += \"_\"  # Display \"_\" instead of \" \" in live_text\n",
    "\n",
    "            elif sign == \"DEL\":\n",
    "                live_text = live_text[:-1]  # Removes the last character\n",
    "\n",
    "            elif sign != \"NOTHING\":\n",
    "                live_text += sign  # Appends the recognized gesture to live_text\n",
    "\n",
    "        \n",
    "        # Set last to be the top_class\n",
    "        last = top_class\n",
    "        \n",
    "        # Set the noise counter to 0 since this is not noise\n",
    "        noise_count = 0 \n",
    "    \n",
    "    # If the confidence of the current sign is not enough for the threshold increase noise counter\n",
    "    else:\n",
    "        \n",
    "        noise_count += 1\n",
    "    \n",
    "    # If there are three consecutive insignificant signs\n",
    "    # Reset count, allowing another consecutive sign to be registerred for instance (A,A)\n",
    "    # Reset the noise counter\n",
    "    if noise_count == 3:\n",
    "        count = 0\n",
    "        noise_count = 0\n",
    "    \n",
    "    # Visualize the results on the frame\n",
    "    #annotated_frame = results[0].plot()\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    \n",
    "    # Example from: https://www.geeksforgeeks.org/python-opencv-write-text-on-video/\n",
    "    cv.putText(frame,  \n",
    "                live_text,  \n",
    "                (10, 460),  \n",
    "                cv.FONT_HERSHEY_SIMPLEX, 1,  \n",
    "                (0, 255, 255),  \n",
    "                2,  \n",
    "                cv.LINE_4) \n",
    "    \n",
    "    cv.imshow(\"Capture\", frame)\n",
    "    \n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        \n",
    "        if DEBUG == True:\n",
    "            \n",
    "            print(\"\\n\\nCaptured Text:\")\n",
    "        \n",
    "            for i in range(len(captured_text)):\n",
    "                \n",
    "                print(\"Translated text: \", captured_text[i] , \" Confidence: \", captured_confidence[i].item())\n",
    "        \n",
    "        # now I replaced the '_' with ' ' when converted to text\n",
    "        spoken_text = ''.join(captured_text).replace(\"_\", \" \").strip()\n",
    "        print(\"Speaking: \", spoken_text)\n",
    "        engine.say(spoken_text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "        break\n",
    "        \n",
    "        \n",
    " \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# a function to replay the captured text after ending the webcam feed\n",
    "while True:\n",
    "    replay_choice = input(\"Press 'R' to replay the sentence or 'E' to exit: \").strip().lower()\n",
    "    if replay_choice == 'r':\n",
    "        print(\"Replaying: \", spoken_text)\n",
    "        engine.say(spoken_text)\n",
    "        engine.runAndWait()\n",
    "    elif replay_choice == 'e':\n",
    "        print(\"Exiting.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Courtesy of: https://www.geeksforgeeks.org/essential-opencv-functions-to-get-started-into-computer-vision/\n",
    "\n",
    "# Mapping each letter+space to the corresponding ASL image path\n",
    "asl_image_paths = {\n",
    "    'A': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\A.jpg',\n",
    "    'B': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\B.jpg',\n",
    "    'C': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\C.jpg',\n",
    "    'D': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\D.jpg',\n",
    "    'E': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\E.jpg',\n",
    "    'F': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\F.jpg',\n",
    "    'G': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\G.jpg',\n",
    "    'H': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\H.jpg',\n",
    "    'I': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\I.jpg',\n",
    "    'J': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\J.jpg',\n",
    "    'K': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\K.jpg',\n",
    "    'L': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\L.jpg',\n",
    "    'M': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\M.jpg',\n",
    "    'N': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\N.jpg',\n",
    "    'O': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\O.jpg',\n",
    "    'P': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\P.jpg',\n",
    "    'Q': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Q.jpg',\n",
    "    'R': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\R.jpg',\n",
    "    'S': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\S.jpg',\n",
    "    'T': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\T.jpg',\n",
    "    'U': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\U.jpg',\n",
    "    'V': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\V.jpg',\n",
    "    'W': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\W.jpg',\n",
    "    'X': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\X.jpg',\n",
    "    'Y': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Y.jpg',\n",
    "    'Z': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\Z.jpg',\n",
    "    '_': r'C:\\Users\\jigsa\\Downloads\\ASL TTS Dataset\\SPACE.jpg'\n",
    "}\n",
    "\n",
    "# a dictionary to store loaded and resized images to avoid loading each time\n",
    "asl_images = {}\n",
    "\n",
    "# pre-load and resize all images to 500x500\n",
    "for letter, path in asl_image_paths.items():\n",
    "    img = cv.imread(path)\n",
    "    if img is not None:\n",
    "        resized_img = cv.resize(img, (500, 500))\n",
    "        asl_images[letter] = resized_img\n",
    "    else:\n",
    "        print(f\"The image for '{letter}' is not found at : {path}\")\n",
    "\n",
    "# a function to display each letter as an ASL gesture\n",
    "def display_asl_gesture(text):\n",
    "    for char in text:\n",
    "        # again, convert the space (' ') to '_' for the dictionary\n",
    "        char = '_' if char == ' ' else char.upper()\n",
    "\n",
    "        # get the pre-loaded and resized ASL image\n",
    "        img = asl_images.get(char)\n",
    "        if img is not None:\n",
    "            cv.imshow(f\"ASL Gesture for {char}\", img)\n",
    "            cv.waitKey(1000)  #set the display time for each image to 1 second\n",
    "            cv.destroyWindow(f\"ASL Gesture for {char}\")\n",
    "        else:\n",
    "            print(f\"ASL gesture for '{char}' is not available.\")\n",
    "\n",
    "# input text to be translated to ASL\n",
    "input_text = input(\"Enter text to translate to ASL gestures: \").strip()\n",
    "\n",
    "# call the display ASL function\n",
    "display_asl_gesture(input_text)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "Recognized Text: hi my name is Andrew\n"
     ]
    }
   ],
   "source": [
    "# Courtesy of: https://pypi.org/project/SpeechRecognition/\n",
    "# and: https://stackoverflow.com/questions/62659602/how-can-i-use-speech-recognition-in-python3-in-the-mac-i-downloaded-but-pyaudio\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def speech_to_text_to_asl():\n",
    "    print(\"Say something...\")\n",
    "\n",
    "    # Start capturing audio\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio_data = recognizer.listen(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize speech using Google's speech recognition\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(f\"Recognized Text: {text}\")\n",
    "\n",
    "            # Call the ASL display function with the recognized text\n",
    "            display_asl_gesture(text)\n",
    "            \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech was unclear. Please try again.\")\n",
    "        except sr.RequestError:\n",
    "            print(\"Could not request results; check your internet connection.\")\n",
    "\n",
    "# Run the speech-to-text-to-ASL function\n",
    "speech_to_text_to_asl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
